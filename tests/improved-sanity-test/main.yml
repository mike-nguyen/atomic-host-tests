---
# vim: set ft=ansible:
#
# !!!NOTE!!! This playbook was tested using Ansible 2.2; it is recommended
# that the same version is used.
#
# This playbook is actually multiple playbooks contained in a single file.
# I had to take this approach, because I encountered difficulties re-using
# the same roles with non-unique variables.  Sometimes the roles would be
# executed with the use of the 'always' tag, sometimes not.
#
# The wrinkle in this approach is that sharing global variables across the
# multiple playbooks requires the use of a file containing the variables
# which is included in each playbook with the 'vars_files' directive.
#
# The sanity tests performed in this set of playbooks:
#   - check permissions on /tmp are correct (RHBZ 1276775) after each reboot
#   - add user to system, verify it is present after upgrade
#   - make changes to /etc, verify changes are present after upgrade
#   - add directory/file to /var, verify they persist after upgrade/rollback
#   - add user post-upgrade, verify it is not present after rollback
#   - add directory/file to /var post-upgrade, verify they persist after
#     rollback
#
# The intent is to expand the sanity test coverage in this set of playbooks
# with small, focused tests going forward.  Additional functional tests that
# are more expansive should be handled in separate playbooks.
#
- name: Improved Sanity Test - Pre-Upgrade
  hosts: "{{ subjects | default('all') }}"
  become: true
  force_handlers: true

  tags:
    - atomic
    - pre_upgrade

  vars_files:
    - "vars/common.yml"

  roles:
    - role: ansible_version_check
      tags:
        - ansible_version_check
        - rhcos

    # Set up handler notification
    - role: handler_notify_on_failure
      handler_name: h_get_journal
      tags:
        - handler_notify_on_failure
        - rhcos

    # Ensure the host(s) are indeed Atomic Host(s)
    - role: atomic_host_check
      tags:
        - atomic_host_check
        - rhcos

    # Before we do anything substantial, let's check the journal to see if
    # anything blew up on boot
    - role: journal_fatal_msgs
      tags:
        - journal_fatal_msgs_first_boot
        - journal_fatal_msgs
        - rhcos

    # https://bugzilla.redhat.com/show_bug.cgi?id=1571264
    - role: ostree_admin_status
      tags:
        - ostree_admin_status
        - rhcos

    # Check RPM signatures
    - role: rpm_signature_verify
      tags:
        - rpm_signature_verify

    # Subscribe if the system is RHEL
    - when: ansible_distribution == 'RedHat'
      role: redhat_subscription
      tags:
        - redhat_subscription

    # The 'archive' option will drop a YAML file into the playbook directory
    # with the value of the original checksum, refspec, and remote
    - role: booted_deployment_set_fact
      archive: true
      tags:
        - booted_deployment_set_fact
        - rhcos

    # Setup local branch and upgrade right away so packages are layered on
    # top of the local branch.  If the local branch is created before the
    # upgrade later in the test, it will only contain the origin content
    # of the ostree.
    - role: local_branch_setup
      branch_name: local-branch
      tags:
        - local_branch_setup
        - rhcos

    # Commit to the local branch
    - role: local_branch_commit
      branch_name: local-branch
      refspec: local-branch
      version: test1
      tags:
        - local_branch_commit
        - rhcos

    # Modify the origin file so we can use rpm-ostree directly
    - role: origin_file_modify
      refspec: local-branch
      tags:
        - origin_file_modify
        - rhcos

    # Do the actual upgrade
    - role: rpm_ostree_upgrade
      tags:
        - rpm_ostree_upgrade_setup
        - rhcos

    # Reboot the host and don't continue until expected services have started
    - role: reboot
      wait_for_services:
        - docker
      tags:
        - reboot_setup
        - rhcos

    # Set some facts that are used in multiple roles
    - role: osname_set_fact
      tags:
        - osname_set_fact
        - rhcos

    # TEST
    # Verify that rpm-ostree can be run as non-root or sudo
    - role: command_privilege_verify
      cmd: rpm-ostree status
      user: bin
      expect_failure: false
      tags:
        - command_privilege_verify
        - rhcos

    # TEST
    # Verify that the output from 'atomic host status'
    # matches 'rpm-ostree status'
    - role: atomic_host_status_verify
      tags:
        - atomic_host_status_verify

    # TEST
    # Verify SELinux labels are correct
    - role: selinux_verify
      tags:
        - selinux_verify
        - rhcos

    # TEST
    # Verify that SELinux file contexts can be modified
    - role: semanage_fcontext_mod
      test_dir_match: "{{ g_semanage_test_dir_match }}"
      test_context: "{{ g_semanage_test_context }}"
      tags:
        - semanage_fcontext_mod
        - rhcos

    # Sets the selinux context of test_dir
    - name: semanage_fcontext_verify
      test_dir: "{{ g_semanage_test_dir }}"
      test_context: "{{ g_semanage_test_context }}"
      tags:
        - semanage_fcontext_verify
        - rhcos

    # Sets boolean defined in vars
    - role: selinux_boolean
      tags:
        - selinux_boolean
        - rhcos

    # TEST
    # Verify that 'firewalld' is correctly installed/configured
    - role: firewalld_service_verify
      tags:
        - firewalld_service_verify
        - rhcos

    # TEST
    # Verify that the Docker storage defaults are correct
    - role: docker_storage_verify
      tags:
        - docker_storage_verify
        - rhcos

    # TEST
    # Verify that 'docker pull' is successful. (Note: this is a different
    # operation than 'atomic pull')
    - role: docker_pull_run_remove
      tags:
        - docker_pull_run_remove
        - rhcos

    # TEST
    # Validate 'atomic pull', 'atomic scan' works correctly.
    # Remove images after test of each command.
    # Do this ahead of 'ostree admin unlock' because of overlay + SELinux
    # incompatibility on RHEL. (This should be fixed in 7.4)
    - role: atomic_pull_verify
      tags:
        - atomic_pull_verify

    # Clean up docker images
    - role: docker_remove_all
      tags:
        - docker_remove_all
        - rhcos

    # TEST
    # Validate atomic scan command
    - when: ansible_distribution == 'RedHat'
      role: atomic_scan_verify
      tags:
        - atomic_scan_verify

    # Clean up docker images
    - role: docker_remove_all
      tags:
        - docker_remove_all
        - rhcos

    # Install package using package layering
    - role: rpm_ostree_install
      roi_packages: "{{ g_pkg }}"
      roi_reboot: false
      tags:
        - rpm_ostree_install

    # Reboot the host and don't continue until expected services have started
    - role: reboot
      wait_for_services:
        - docker
      tags:
        - reboot_pre_up_install

    # TEST
    # Verify that the package layering worked
    - role: rpm_ostree_install_verify
      roiv_package_name: "{{ g_pkg }}"
      roiv_binary_name: "{{ g_pkg }}"
      tags:
        - rpm_ostree_install_verify

    # TEST
    # Verify admin unlock by installing an RPM
    - role: ostree_admin_unlock_hotfix
      tags:
        - ostree_admin_unlock_hotfix
        - rhcos

    # TEST
    # Verify overlayfs is present
    - role: overlayfs_verify_present
      tags:
        - overlayfs_verify_present
        - rhcos

    # Install an rpm using the rpm command
    - role: rpm_install
      rpm_path: "{{ g_rpm_path }}"
      tags:
        - rpm_install
        - rhcos

    # TEST
    # Verify the rpm installed successfully when unlocked
    - role: package_verify_present
      rpm_name: "{{ g_rpm_name }}"
      check_binary: false
      tags:
        - package_verify_present
        - rhcos

    # TEST
    # Verify that /tmp has the proper permissions
    - role: tmp_check_perms
      tags:
        - tmp_check_perms
        - rhcos

    # TEST
    # Verify that the RPM database is functional
    - role: rpmdb_verify
      tags:
        - rpmdb_verify
        - rhcos

    # Add users, make changes to /etc, and add things to /var before the
    # the system is upgraded
    - role: user_add
      ua_uid: "{{ g_uid1 }}"
      tags:
        - user_add
        - rhcos

    # TEST
    # Verify the user created is present
    - role: user_verify_present
      uvp_uid: "{{ g_uid1 }}"
      tags:
        - user_verify_present
        - rhcos

    # Modify etc
    - role: etc_modify
      tags:
        - etc_modify
        - rhcos

    # TEST
    # Verify that the changes to etc persisted
    - role: etc_verify_changes
      tags:
        - etc_verify_changes
        - rhcos

    # Add files to var
    - role: var_add_files
      vaf_filename: "{{ g_file1 }}"
      vaf_dirname: "{{ g_dir1 }}"
      tags:
        - var_add_files
        - rhcos

    # TEST
    # Verify the files added to var persisted
    - role: var_files_present
      vfp_filename: "{{ g_file1 }}"
      vfp_dirname: "{{ g_dir1 }}"
      tags:
        - var_files_present
        - rhcos

    # Pull down docker base images, build a layered image, run it, then
    # remove the container.  But we keep the image around to run later.
    # Run 'atomic run/stop' tests prior to running 'docker run' test so
    # the built images can be run later.
    - role: docker_pull_base_image
      tags:
        - docker_pull_base_image
        - rhcos

    # Build a local httpd image
    - role: docker_build
      db_src: "roles/docker_build/files/{{ g_osname }}/httpd/"
      db_image_name: "{{ g_httpd_name }}"
      tags:
        - docker_build

    # TEST
    # Verify we can run a container via atomic
    - role: atomic_run_verify
      container: "{{ g_httpd_name }}"
      tags:
        - atomic_run_verify

    # TEST
    # Verify we can stop a container via atomic
    - role: atomic_stop_verify
      container: "{{ g_httpd_name }}"
      tags:
        - atomic_stop_verify

    #####
    # TEST
    # Verify we can remove the httpd container and it is no longer running
    - role: docker_rm_container
      drc_container_name: "{{ g_httpd_name }}"
      tags:
        - docker_rm_container

    # TEST
    # Verify we can run the container again
    - role: docker_run
      dr_image_name: "{{ g_httpd_name }}"
      dr_run_options: "-d -p 80:80"
      tags:
        - docker_run

    # TEST
    # Verify port 80 is open and we can retrieve the URL
    - role: check_open_port
      cop_port: "80"
      cop_url: "http://localhost:80"
      tags:
        - check_open_port

    # TEST
    # Verify we can remove the httpd container and it is no longer running
    - role: docker_rm_container
      drc_container_name: "{{ g_httpd_name }}"
      tags:
        - docker_rm_container_again

    # Configure container_manage_cgroup boolean
    # See - https://bugzilla.redhat.com/show_bug.cgi?id=1510139
    #     - https://bugzilla.redhat.com/show_bug.cgi?id=1554881
    - when: ansible_distribution == "RedHat" or
            ansible_distribution == "Fedora"
      role: selinux_boolean
      g_seboolean: "container_manage_cgroup"
      tags:
        - container_manage_cgroup_bool
        - non_priv_systemd_httpd_container
        - rhcos

    # TEST
    # Build non-priviledged systemd httpd container
    - role: docker_build
      db_src: "roles/docker_build/files/{{ g_osname }}/systemd_httpd/"
      db_image_name: "{{ g_osname }}_systemd_httpd"
      tags:
        - docker_build_systemd
        - non_priv_systemd_httpd_container

    # TEST
    # Run non-privileged systemd httpd container
    - role: docker_run
      dr_image_name: "{{ g_osname }}_systemd_httpd"
      dr_run_options: "-d -p 80:80"
      tags:
        - docker_run_systemd
        - non_priv_systemd_httpd_container

    # TEST
    # Verify that port 80 is open and the URL is accessible
    - role: check_open_port
      cop_port: "80"
      cop_url: "http://localhost:80/"
      tags:
        - check_open_port_systemd
        - non_priv_systemd_httpd_container

    # TEST
    # Remove the non-privileged systemd httpd container
    - role: docker_rm_container
      drc_container_name: "{{ g_osname }}_systemd_httpd"
      tags:
        - docker_rm_container_systemd
        - non_priv_systemd_httpd_container

    # TEST
    # Install, run and uninstall cockpit using atomic command
    - role: atomic_installation_verify
      tags:
        - atomic_installation_verify

    # new commit
    - role: local_branch_commit
      branch_name: local-branch
      refspec: local-branch
      version: test2
      tags:
        - local_branch_commit
        - rhcos
      check_mode: false

    # TEST
    # Verify installation of etcd system container works
    - role: etcd_sys_container_install
      tags:
        - etcd_sys_container_install

    # Upgrade host
    - role: rpm_ostree_upgrade
      tags:
        - rpm_ostree_upgrade
        - rhcos
      check_mode: false

    # Before we reboot, check the journal for anything that blew up
    - role: journal_fatal_msgs
      tags:
        - journal_fatal_msgs_reboot
        - journal_fatal_msgs
        - rhcos

    - role: reboot
      wait_for_services:
        - docker
      tags:
        - reboot_pre_upgrade
        - rhcos

# ---

- name: Improved Sanity Test - Post-Upgrade
  hosts: "{{ subjects | default('all') }}"
  become: true
  force_handlers: true

  tags:
    - atomic
    - post_upgrade

  vars_files:
    - "vars/common.yml"
    - ["vars/{{ ansible_distribution|lower }}.yml", "vars/os_defaults.yml"]

  roles:
    - role: ansible_version_check
      tags:
        - ansible_version_check
        - rhcos

    - role: handler_notify_on_failure
      handler_name: h_get_journal
      tags:
        - handler_notify_on_failure
        - rhcos
      check_mode: false

    - role: atomic_host_check
      tags:
        - atomic_host_check
        - rhcos

    # Setup facts again. (Need to use the 'check_mode: false' option to ensure the
    # role runs again)
    - role: osname_set_fact
      tags:
        - osname_set_fact
        - rhcos
      check_mode: false

    # Before we do any additional actions, let's check the journal to see if
    # anything blew up after reboot
    - role: journal_fatal_msgs
      tags:
        - journal_fatal_msgs_second_boot
        - journal_fatal_msgs
        - rhcos

    # https://bugzilla.redhat.com/show_bug.cgi?id=1571264
    - role: ostree_admin_status
      tags:
        - ostree_admin_status_again
        - rhcos

    # We remove any subscriptions after the upgrade to verify that
    # 'rpm-ostree status' with the 'unconfigured-state' field present.
    # https://bugzilla.redhat.com/show_bug.cgi?id=1421867
    - when: ansible_distribution == 'RedHat'
      role: redhat_unsubscribe
      unconfigured_state: true
      tags:
        - redhat_unsubscribe

    # Verify that rpm-ostree can be run as non-root or sudo
    - role: command_privilege_verify
      cmd: rpm-ostree status
      user: bin
      expect_failure: false
      tags:
        - command_privilege_verify
        - rhcos

    # Compare 'atomic host status' and 'rpm-ostree status' again
    - role: atomic_host_status_verify
      stop_daemon: true
      tags:
        - atomic_host_status_verify

    # Re-subscribe
    - when: ansible_distribution == 'RedHat'
      role: redhat_subscription
      tags:
        - redhat_subscription

    # Verify correct SELinux labels again
    - role: selinux_verify
      tags:
        - selinux_verify
        - rhcos

    # Verify the change to the SELinux file context persisted
    - role: semanage_fcontext_verify
      test_dir: "{{ g_semanage_test_dir }}"
      test_context: "{{ g_semanage_test_context }}"
      tags:
        - semanage_fcontext_verify
        - rhcos

    # Verify the SELinux boolean changes
    - role: selinux_boolean_verify
      tags:
        - selinux_boolean_verify
        - rhcos

    # TEST
    # Verify that 'firewalld' is correctly installed/configured
    - role: firewalld_service_verify
      tags:
        - firewalld_service_verify
        - rhcos

    # Verify that the Docker storage defaults haven't changed
    - role: docker_storage_verify
      tags:
        - docker_storage_verify
        - rhcos

    - role: docker_pull_run_remove
      tags:
        - docker_pull_run_remove
        - rhcos

    # Check layered package is still installed
    - role: rpm_ostree_install_verify
      roiv_package_name: "{{ g_pkg }}"
      roiv_binary_name: "{{ g_pkg }}"
      tags:
        - rpm_ostree_install_verify

    # Check etcd system container is still running
    - role: command
      cmd: systemctl status etcd
      output: 'active (running)'
      tags:
        - command

    # Uninstall etcd system container
    - role: atomic_system_uninstall
      asu_name: etcd
      tags:
        - atomic_system_uninstall

    # Verify etcd system container uninstall
    - role: atomic_system_uninstall_verify
      asuv_image: etcd
      tags:
        - atomic_system_uninstall_verify

    # Install etcd system container
    - role: etcd_sys_container_install
      delete_key: true
      tags:
        - etcd_sys_container_install
        - cloud_image
      check_mode: false

    # Uninstall etcd system container
    - role: atomic_system_uninstall
      asu_name: etcd
      tags:
        - atomic_system_uninstall
        - cloud_image
      check_mode: false

    # Verify etcd system container uninstall
    - role: atomic_system_uninstall_verify
      asuv_image: etcd
      tags:
        - atomic_system_uninstall_verify
        - cloud_image

    # Install a package via rpm-ostree
    - role: rpm_ostree_install
      roi_packages: "{{ g_ros_install_pkg }}"
      roi_reboot: false
      tags:
        - rpm_ostree_install

    # Reboot the host and don't continue until expected services have started
    - role: reboot
      tags:
        - reboot_post_up_install

    # TEST
    # Verify that the previously installed package is present
    - role: rpm_ostree_install_verify
      roiv_package_name: "{{ g_pkg }}"
      roiv_binary_name: "{{ g_pkg }}"
      tags:
        - rpm_ostree_install_verify

    # TEST
    # Verify that the rpm-ostree installed package is present
    - role: rpm_ostree_install_verify
      roiv_package_name: "{{ g_ros_install_pkg }}"
      roiv_binary_name: "{{ g_ros_install_bin }}"
      tags:
        - rpm_ostree_install_verify

    # Start httpd
    - when: ansible_distribution != 'CentOS'
      role: httpd_start
      httpd_port: 8080
      tags:
        - httpd_start

    # Verify that httpd is serving on port 8080
    - when: ansible_distribution != 'CentOS'
      role: url_verify
      url: http://localhost:8080
      destination: /dev/null
      tags:
        - url_verify

    # TEST
    # Check admin unlock overlayfs is no longer there after upgrade
    - role: overlayfs_verify_missing
      tags:
        - overlayfs_verify_missing
        - rhcos

    # TEST
    # Verify admin unlocked package is removed after upgrade
    - role: package_verify_missing
      rpm_name: "{{ g_rpm_name }}"
      tags:
        - package_verify_missing
        - rhcos

    # TEST
    # Check that /tmp is properly setup again
    - role: tmp_check_perms
      tags:
        - tmp_check_perms
        - rhcos

    # TEST
    # Check that the RPM database is functional again
    - role: rpmdb_verify
      tags:
        - rpmdb_verify
        - rhcos

    # TEST
    # Verify that the new users, /etc changes, and /var additions are still
    # present after the upgrade
    - role: user_verify_present
      uvp_uid: "{{ g_uid1 }}"
      tags:
        - user_verify_present
        - rhcos

    - role: etc_verify_changes
      tags:
        - etc_verify_changes
        - rhcos

    - role: var_files_present
      vfp_filename: "{{ g_file1 }}"
      vfp_dirname: "{{ g_dir1 }}"
      tags:
        - var_files_present
        - rhcos

    # Add more users and more files to /var ahead of the rollback
    - role: user_add
      ua_uid: "{{ g_uid2 }}"
      tags:
        - user_add
        - rhcos

    # TEST
    # Ensure the user we just added is present
    - role: user_verify_present
      uvp_uid: "{{ g_uid2 }}"
      tags:
        - user_verify_present
        - rhcos

    # Add some files to var
    - role: var_add_files
      vaf_filename: "{{ g_file2 }}"
      vaf_dirname: "{{ g_dir2 }}"
      tags:
        - var_add_files
        - rhcos

    # TEST
    # Ensure the files we added var are present
    - role: var_files_present
      vfp_filename: "{{ g_file2 }}"
      vfp_dirname: "{{ g_dir2 }}"
      tags:
        - var_files_present
        - rhcos

    # TEST
    # Install, run and uninstall cockpit using atomic command
    - role: atomic_installation_verify
      tags:
        - atomic_installation_verify

    # TEST
    # Run the previously created docker layered image, then remove
    # the container and the image.
    - role: atomic_run_verify
      container: "{{ g_httpd_name }}"
      tags:
        - atomic_run_verify

    # TEST
    # Ensure we are able to stop the httpd container
    - role: atomic_stop_verify
      container: "{{ g_httpd_name }}"
      tags:
        - atomic_stop_verify

    # Remove the httpd container
    - role: docker_rm_container
      drc_container_name: "{{ g_httpd_name }}"
      tags:
        - docker_rm_container

    # Remove the httpd image
    - role: docker_rmi
      drmi_image_name: "{{ g_httpd_name }}"
      tags:
        - docker_rmi

    # TEST
    # Verify we can start the non-privileged systemd httpd container
    - role: docker_run
      dr_image_name: "{{ g_osname }}_systemd_httpd"
      dr_run_options: "-d -p 80:80"
      tags:
        - docker_run_systemd
        - non_priv_systemd_httpd_container

    # Verify that port 80 is open and URL is accessible
    - role: check_open_port
      cop_port: "80"
      cop_url: "http://localhost:80/"
      tags:
        - check_open_port_systemd
        - non_priv_systemd_httpd_container

    # Remove the systemd httpd container
    - role: docker_rm_container
      drc_container_name: "{{ g_osname }}_systemd_httpd"
      tags:
        - docker_rm_container_systemd
        - non_priv_systemd_httpd_container

    # Remove all things docker
    - role: docker_remove_all
      tags:
        - docker_remove_all
        - rhcos

    # TEST
    # Validate atomic pull works
    - role: atomic_pull_verify
      tags:
        - atomic_pull_verify

    # Clean up docker files
    - role: docker_remove_all
      tags:
        - docker_remove_all
        - rhcos

    # TEST
    # Validate atomic scan works
    - when: ansible_distribution == 'RedHat'
      role: atomic_scan_verify
      tags:
        - atomic_scan_verify

    # Rollback and reboot
    - role: rpm_ostree_rollback
      tags:
        - rpm_ostree_rollback
        - rhcos

    # Before we reboot, check the journal for anything that blew up
    - role: journal_fatal_msgs
      tags:
        - journal_fatal_msgs_second_reboot
        - journal_fatal_msgs
        - rhcos

    # Reboot the host and don't continue until expected services have started
    - role: reboot
      wait_for_services:
        - docker
      tags:
        - reboot_post_upgrade
        - rhcos

# ---

- name: Improved Sanity Test - Post-Rollback
  hosts: "{{ subjects | default('all') }}"
  become: true
  force_handlers: true

  tags:
    - atomic
    - post_rollback

  vars_files:
    - "vars/common.yml"
    - ["vars/{{ ansible_distribution|lower }}.yml", "vars/os_defaults.yml"]

  roles:
    - role: ansible_version_check
      tags:
        - ansible_version_check
        - rhcos

    # Set up handler notification
    - role: handler_notify_on_failure
      handler_name: h_get_journal
      tags:
        - handler_notify_on_failure
        - rhcos
      check_mode: false

    # Ensure the host(s) are indeed Atomic Host(s)
    - role: atomic_host_check
      tags:
        - atomic_host_check
        - rhcos

    # Before we do any additional actions, let's check the journal to see if
    # anything blew up after reboot
    - role: journal_fatal_msgs
      tags:
        - journal_fatal_msgs_third_boot
        - journal_fatal_msgs
        - rhcos

    - role: selinux_verify
      tags:
        - selinux_verify
        - rhcos

    # TEST
    # Verify that the change to the SELinux file context still exists in
    # the original deployment
    - role: semanage_fcontext_verify
      test_dir: "{{ g_semanage_test_dir }}"
      test_context: "{{ g_semanage_test_context }}"
      tags:
        - semanage_fcontext_verify
        - rhcos

    # TEST
    # Check layered package is still installed
    - role: rpm_ostree_install_verify
      roiv_package_name: "{{ g_pkg }}"
      roiv_binary_name: "{{ g_pkg }}"
      tags:
        - rpm_ostree_install_verify

    # TEST
    # Check second package is not installed
    - role: rpm_ostree_uninstall_verify
      rouv_package_name: "{{ g_ros_install_pkg }}"
      rouv_binary_name: "{{ g_ros_install_bin }}"
      tags:
        - rpm_ostree_uninstall_verify

    # TEST
    # Check admin unlock overlayfs missing
    - role: overlayfs_verify_missing
      tags:
        - overlayfs_verify_missing

    # TEST
    # Check that /tmp is properly setup yet again
    - role: tmp_check_perms
      tags:
        - tmp_check_perms
        - rhcos

    # TEST
    # Check that the RPM database is functional yet again
    - role: rpmdb_verify
      tags:
        - rpmdb_verify
        - rhcos

    # TEST
    # Verify changes still exist in original tree
    - role: etc_verify_changes
      tags:
        - etc_verify_changes
        - rhcos

    # TEST
    # Verify first user added is still present in original tree
    - role: user_verify_present
      uvp_uid: "{{ g_uid1 }}"
      tags:
        - user_verify_present
        - rhcos

    # TEST
    # Verify first files dropped in /var are still in original tree
    - role: var_files_present
      vfp_filename: "{{ g_file1 }}"
      vfp_dirname: "{{ g_dir1 }}"
      tags:
        - var_files_present
        - rhcos

    # TEST
    # Verify that the newest user is not present in the previous tree
    - role: user_verify_missing
      uvm_uid: "{{ g_uid2 }}"
      tags:
        - user_verify_missing
        - rhcos

    # TEST
    # Verify the most recent changes to /var are present
    - role: var_files_present
      vfp_filename: "{{ g_file2 }}"
      vfp_dirname: "{{ g_dir2 }}"
      tags:
        - var_files_present
        - rhcos

    # TEST
    # use 'all' keyword to remove all installed pacakges
    - role: rpm_ostree_uninstall
      rou_packages: all
      rou_reboot: true
      tags:
        - rpm_ostree_uninstall
        - rhcos

    # load in the YAML files with archived deployment info
    - role: load_variables
      filename: "{{ playbook_dir}}/{{ ansible_host }}_bdsf_archive.yml"
      tags:
        - load_variables
        - rhcos

    # rebase back to original checkout
    - role: rpm_ostree_rebase
      ror_remote_name: "{{ g_archive_remote_name }}"
      ror_refspec: "{{ g_archive_refspec }}"
      ror_commit: "{{ g_archive_checksum }}"
      tags:
        - rpm_ostree_rebase
        - rhcos

    # Reboot the host and don't continue until expected services have started
    - role: reboot
      wait_for_services:
        - docker
      tags:
        - reboot_cleanup
        - rhcos

    # cleanup any extra deployments
    - role: rpm_ostree_cleanup_all
      tags:
        - rpm_ostree_cleanup_all
        - rhcos

    # remove any docker artifacts
    - role: docker_remove_all
      tags:
        - docker_remove_all
        - rhcos

    # remove users
    - role: user_del
      ud_users:
        - "{{ g_uid1 }}"
        - "{{ g_uid2 }}"
      tags:
        - user_del
        - rhcos

    # cleanup our subscriptions because we are nice people
    - when: ansible_distribution == 'RedHat'
      role: redhat_unsubscribe
      tags:
        - redhat_unsubscribe

    # As the final action, check the journal for any fatal problems
    - role: journal_fatal_msgs
      tags:
        - journal_fatal_msgs_final
        - journal_fatal_msgs
        - rhcos
